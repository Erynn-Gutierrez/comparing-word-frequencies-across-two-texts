{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458d7af5",
   "metadata": {},
   "source": [
    "# Comparing Word Frequencies Across Two Texts\n",
    "## Introduction\n",
    "In this lesson, you’ll learn how to compare the most common words in two different texts using Python. This is a foundational technique in digital text analysis and can help you spot patterns, themes, or stylistic differences between books, chapters, or authors.\n",
    "\n",
    "We’ll use two sample texts from Project Gutenberg: *Alice’s Adventures in Wonderland* by Lewis Carroll and *The Adventures of Sherlock Holmes* by Arthur Conan Doyle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddb6fa",
   "metadata": {},
   "source": [
    "### Downloading the Texts\n",
    "\n",
    "Here, we use the requests library to download the full text of \"Alice's Adventures in Wonderland\" and \"The Adventures of Sherlock Holmes\" from Project Gutenberg. It stores the text of each book as a string in the variables alice_text and sherlock_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33045001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URLs for the texts\n",
    "url_alice = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "url_sherlock = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "\n",
    "# Download and decode\n",
    "alice_text = requests.get(url_alice).text\n",
    "sherlock_text = requests.get(url_sherlock).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5fa19",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning and Tokenization\n",
    "Here, we define a function clean_and_tokenize to preprocess the raw text. First, it normalizes curly quotes and apostrophes to standard ones, converts the text to lowercase, and then uses a regular expression (re.findall) to extract all words, including those with internal apostrophes (contractions). Finally, it removes any single-letter tokens (like 't'), except for 'i' and 'a', which are valid English words. The function is then applied to both alice_text and sherlock_text, storing the resulting lists of cleaned words in alice_words and sherlock_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34a74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Normalize curly quotes and apostrophes to straight ones\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    text = text.lower()\n",
    "    # Regex to match words with optional internal apostrophes (e.g. don't, it's)\n",
    "    words = re.findall(r\"\\b[a-z]+(?:'[a-z]+)?\\b\", text)\n",
    "    # Optionally remove single-letter tokens except 'i' and 'a'\n",
    "    words = [word for word in words if len(word) > 1 or word in ('i', 'a')]\n",
    "    return words\n",
    "\n",
    "alice_words = clean_and_tokenize(alice_text)\n",
    "sherlock_words = clean_and_tokenize(sherlock_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165017e",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "In a list, we define common English stopwords - words that are frequent but don't carry much meaning - and then create a function remove_stopwords to eliminate these words from the tokenized lists.\n",
    "\n",
    "The function uses a list comprehension to return a new list containing only the words that are not in the stopwords set. This is applied to both alice_words and sherlock_words, saving the results as alice_words_clean and sherlock_words_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32361336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple stopword list\n",
    "stopwords = set([\n",
    "    'the', 'and', 'to', 'of', 'a', 'i', 'it', 'in', 'or', 'is', 'd', 's', 'as', \n",
    "    'for', 'on', 'with', 'was', 'he', 'she', 'at', 'by', 'an', 'be', 'this', 'that', \n",
    "    'from', 'but', 'not','then', 'are', 'his', 'her', 'they', 'you', 'had', 'have', 'my',\n",
    "    'me', 'we', 'so', 'what', 'there', 'said', 'very', 'him', 'her', 'your', 'were',\n",
    "    'would', 'if', 'when', 'which', 'do', 'been',  'could', 'about', 'them', 'has', 'into',\n",
    "    'know', 'like', 'herself','went', 'again', 'up', 'out', 'all', 'one', 'no',\n",
    "    'some', 'upon', 'will', 'who', 'now', 'our', 'how', 'am', 'did', 'us'\n",
    "])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "alice_words_clean = remove_stopwords(alice_words)\n",
    "sherlock_words_clean = remove_stopwords(sherlock_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22620f",
   "metadata": {},
   "source": [
    "### Counting Word Frequencies\n",
    "We'll use Python's collections.Counter class to efficiently count the frequency of each word in the cleaned word lists. Counter automatically creates a dictionary-like object where keys are the unique words and values are their counts. The word counts for each text are stored in alice_counts and sherlock_counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ee8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "alice_counts = Counter(alice_words_clean)\n",
    "sherlock_counts = Counter(sherlock_words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4291ebc",
   "metadata": {},
   "source": [
    "### Displaying the Most Common Words\n",
    "Let's see the top 10 words in each text. We can use the most_common() method of the Counter object to get a list of the 10 most frequent words and their counts, and then print these lists out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acff6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in Alice's Adventures in Wonderland:\n",
      "[('alice', 386), ('little', 127), ('down', 103), ('thought', 74), ('off', 73), ('time', 71), ('queen', 68), ('see', 67), ('well', 63), ('king', 61)]\n",
      "\n",
      "Top 10 words in The Adventures of Sherlock Holmes:\n",
      "[('holmes', 466), ('man', 291), ('mr', 275), ('little', 269), ('see', 232), ('down', 230), ('may', 212), ('should', 211), ('well', 201), ('over', 183)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 words in Alice's Adventures in Wonderland:\")\n",
    "print(alice_counts.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in The Adventures of Sherlock Holmes:\")\n",
    "print(sherlock_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f1d24",
   "metadata": {},
   "source": [
    "### Comparing the Top Words\n",
    "Which words are common to both texts? Which are unique? Feel free to compare texts of your choosing, and see what the results bring to light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88bb1f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words common to both texts: {'well', 'see', 'little', 'down'}\n",
      "Words unique to Alice: {'alice', 'off', 'time', 'king', 'queen', 'thought'}\n",
      "Words unique to Sherlock: {'man', 'mr', 'over', 'holmes', 'should', 'may'}\n"
     ]
    }
   ],
   "source": [
    "alice_top_words = set([word for word, count in alice_counts.most_common(10)])\n",
    "sherlock_top_words = set([word for word, count in sherlock_counts.most_common(10)])\n",
    "\n",
    "common_words = alice_top_words & sherlock_top_words\n",
    "alice_unique = alice_top_words - sherlock_top_words\n",
    "sherlock_unique = sherlock_top_words - alice_top_words\n",
    "\n",
    "print(\"Words common to both texts:\", common_words)\n",
    "print(\"Words unique to Alice:\", alice_unique)\n",
    "print(\"Words unique to Sherlock:\", sherlock_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
